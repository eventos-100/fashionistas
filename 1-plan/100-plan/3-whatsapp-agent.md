## Understanding Ava: Memory and Retrieval-Augmented Generation

### Introduction

In this lesson, we delve into the intricate workings of Ava, the WhatsApp agent, focusing on her memory systems and the concept of Retrieval-Augmented Generation (RAG). This summary will explore how Ava utilizes both short-term and long-term memory, the role of vector databases, and the implementation of these concepts in code. By the end, you will have a comprehensive understanding of how Ava processes and retains information to enhance user interactions.

---

### Memory Systems in Ava

Ava's memory is divided into two primary types: **short-term memory** and **long-term memory**. 

#### Short-Term Memory

- **Storage**: Utilizes MySQL and runs locally or in a Docker instance.
- **Content**: 
  - Summary of the conversation.
  - Workflow selected by the router (e.g., conversation type).
  - Recent messages exchanged with the user.
  - Current activity and context from long-term memory.

The short-term memory captures the immediate context of conversations, allowing Ava to respond appropriately based on recent interactions.

#### Long-Term Memory

- **Storage**: Implemented using Quadrant, a vector database.
- **Functionality**: 
  - Extracts relevant memories from conversations.
  - Stores key user details (e.g., name, profession, preferences).
  
Ava's long-term memory is crucial for maintaining continuity in conversations, enabling her to recall important user information over time.

### Retrieval-Augmented Generation (RAG)

RAG is a three-step process that enhances Ava's ability to provide contextually relevant responses:

1. **Retrieve**: Ava retrieves relevant memories based on user queries.
2. **Augment**: Injects these memories into the conversation context.
3. **Generate**: Uses a language model to formulate a response.

#### Embedding Vectors and Semantic Search

- **Embedding Vectors**: Numerical representations of text, images, or audio that capture semantic meaning.
- **Semantic Search**: A technique to find the closest vectors in a high-dimensional space, allowing Ava to identify relevant memories based on user queries.

| **Concept**          | **Description**                                                                 |
|----------------------|---------------------------------------------------------------------------------|
| **Embedding Model**  | Converts objects into meaningful vector representations.                        |
| **Vector Database**  | Manages indexing, retrieving, and querying embedding vectors.                  |
| **Semantic Meaning** | The significance of words or phrases in context, aiding in accurate memory retrieval. |

### Implementation in Code

The implementation of Ava's memory systems involves several key components:

#### Short-Term Memory Code

- **Graph State**: Captures the current state of the conversation.
- **Async SQL Saver**: Persists recent messages and conversation context.

```python
class AICompanionState:
    # Code to manage short-term memory
```

#### Long-Term Memory Code

- **Memory Manager**: Extracts and stores relevant memories.
- **Memory Injection**: Retrieves memories from the vector database and injects them into the conversation context.

```python
def analyze_memory(message):
    # Code to analyze and store important user details
```

### Vector Database Providers

Choosing the right vector database is essential for effective memory management. Key considerations include:

- **Performance**: Speed of semantic search.
- **Open Source**: Preference for open-source solutions.
- **Self-Hosting**: Ability to host the database on personal servers.

#### Why Quadrant?

Ava's team chose Quadrant for several reasons:

1. **Open Source**: Promotes transparency and community collaboration.
2. **Self-Hosting**: Offers flexibility in deployment.
3. **Free Tier**: Provides a cost-effective solution for developers.

### Conclusion

In summary, Ava's memory systems and the RAG framework are integral to her functionality as a conversational agent. By effectively utilizing short-term and long-term memory, along with advanced techniques like semantic search and embedding vectors, Ava can provide personalized and contextually relevant responses. Understanding these concepts not only enhances the development of AI applications but also improves user experience by creating more engaging and meaningful interactions.

---

This analytical summary encapsulates the key elements of Ava's memory systems and the RAG framework, providing a clear understanding of their implementation and significance in enhancing user interactions.

hello everybody and welcome to lesson three of Ava the WhatsApp agent in this lesson we're going to cover a couple of things that I have already here written so first of all we going to check we're going to understand how Ava uses her memory both the shortterm and the long-term memory then I'm going to introduce you to the concept of rag I think one of the most important Concepts in gen applications and it's I think it's used in most applications in one way or another um as part of this rack we are going to explain you what is a vector database and I'm going to explain you all the different database providers so the vector database providers that there are and why we chose to work with quadrant then we are going to dig deep into the code and actually we're going to understand how the shortterm memory is implemented using my SQL SQL and also how the long-term memory is implemented using quadrant and finally I'm going to show you how you can set up your own instance of quadrant Cloud because actually they have a free tier that allows you to create your own um free cluster using Google Cloud okay so suing in we have the first diagram that Miguel has prepared and as always it's looking beautiful this is a really cool diagram because it shows both the shortterm and the longterm memory in the same kind of like picture and I think you will understand it very very very well so let's start by looking at the shortterm memory that we have represented here on the left for storing this shortterm memory we are using S and is and everything is running directly in the in the docker instance or in the local machine and what this shortterm memory stores is actually the graph state that we already reviewed in the previous lesson so if you haven't looked that video you should go and look for lesson two but what we have is a set of parameters that we have selected for our application and those are the summary of the conversation until that point the workflow that has been selected by the router either conversation an image or an audio the audio Buffet itself the image path itself as well the current activity that Ava might be doing and the memory context which refers to those memories that we retrieve from the long-term database plus of course course all those recent messages of the conversation that you're having right now with ABA a conversation that I want to review with you because I think it reflects very very well how we extract those relevant memories and let me come back quickly to the diagram of the previous lesson so this H the graph diagram right in which we represented those nodes and edges that conform our brain or our graph or ABA brain and as you can see here the first note that we have is just the memory extraction right so so just remember that and let's come back to the memory diagram but everything will be super super clear once in a couple of minutes we review the code okay back to the memory diagram and now let's pay attention to the right side part of this diagram so the long-term memory that we Implement with quadrant and by looking at this we're going to start already understanding how we extract those relevant memories so let's start from the top right so we have AA just saying hello I'm AA and now Miguel will be saying hey I'm Miguel nice to meet you so what it starts Happening Here is that AA or or the system that we have built or aa's brain will process each of the messages that Miguel sends or that the user sends understand if there is something relevant so something that is special to this user right something that for example denotes a name or where is he from or what is his profession or what does he like to do um or what is he doing right now in the moment so so this will probably remind you of how chat GPT mes remembers the conversations that you have had with it and right like probably in in chat you have seen that when you tell something important to to chpt for example so for example I'm building this cool machine learning or artificial intelligence project called AA blah blah blah blah and now probably you will see or I will see my messages something like memory updated and what it what chbt is doing is more L what we're seeing here so let's look at at what we have here so AA you will maybe like just have a normal conversation with you and maybe like she will she will ask you hey where you from and Miguel will say I'm Spanish so this means that AA will understand okay Spanish this is relevant I'm Gonna Save this in my long-term memory same thing right like the profession so ma engineer um so you would have a a kind of like completely normal conversation with AA but in the background ABA will be saving those key aspects about yourself and what Miguel has put in in this diagram nicely with the string or or or that piece of text that ABA will get from the message and and then also this array of points or circles so those would be so those would represent H the embedding Vector which are actually the elements that are saved in a vector database and I'm going to explain you this now in the next diagram all right so we have already seen how we extract memories from the recent conversation so more or less we have understand how it's done but we still don't know very well what is an embedding vector or an embedding and now I'm going to explain you how we inject relevant memories and we actually do this thing by using this framework or this concept called retrieval augmented generation and by explaining you this along the way I'm going to explain you and you're going to understand super super well what is an embedding what is an embedding model what is semantic search and what is a vector database so let's get it started so retrieval augmented generation is a three-word concept and as a three-word concept I want you to understand it as a three-step process so first of all retrieve second augment and third generate and let's start now by of course number one retrieve so what happens when Ava um gives you an answer is injecting some relevant memories before giving you the answer right so those memories will be injected let's say in the prompt so those memories that we retrieve are the result of doing something called semantic search and now let's let's go a step by step to understand how we do the semantic search and what it actually represents what it actually means so first of all we have just um a query or a question or just a sentence that is just a string right it's just it's just plain text so what we do is we use an embedding model to generate an embedding or an embedding Vector which is simply a vector representation of an object text image video or audio and by Vector representation we just we just say numerical representation right as we have it here it could be something like 0.5 .4 0.8 0.1 in usually of course in a higher dimensionality than just four but essentially it's just that right it's just a vector so a numerical Vector so what is an embedding model then so an embedding model is a model that converts those objects into meaningful Vector representations and and here let's pay attention to to meaningful and by meaningful let's understand it from the text perspective so it has a semantic meaning right and this concept of semantic meaning H ties very well with the next thing which is semantic search so semantic search is just doing kind of like nearest neighbors for Vector representations a nearest neighbor if you don't know is a technique to find those close elements in regards to a distance and finally last thing from this list what is a vector database then so a vector database is a is also called like a vector search engine and that what it does is it manages so it helps you indexing retrieving and quing these embedding vectors and since I know this might still not be completely clear let's zoom in in what a vector database is so a vector database would have store these high dimensional vectors but now for the sake of this explanation let's just imagine that we have just two dimensions and what I have here represented in green are those relevant memories that we have already extracted in past conversations with the user and in blue we have that query H for which we are doing the semantic search so what we are actually doing is finding in this High dimensional space so here in this simplified version in this two-dimensional space those vectors that are the closest to the query so here the query was um that I would that I could be asking AA right so I could be saying hey remember my favorite movie and ABA before giving an answer it will look into her long-term memory into her into the vector database and it would retrieve those relevant memories so what are so what could be those those that have a similar semantic meaning so let's just look at this sentence right we have remember my favorite movie so what are the things here that have semantic weight so that would be for example remembering favorite or movie right so here we can see on on movie movie and film are semantically Clos because those represent the same thing and similarly um as well here movie and movie of course that's that's even clear and those things are and so these factors are very close by because they are talking as itially about the same thing movies so this way you could imagine also different clusters or different um arations forming by different topics so another topic could be something about food so we have here that ABA has stored in her memory that um the user loves pizza and also he likes to eat healthy could be contradicting but but yeah those things are about eating right so they are close by in in the vector space same thing he's from lanero he lives in Madrid so this things could be related to um related to to a space or to cities and those things are more so these things these two things have more things in common than pizza and lerot so this more less would be the whole idea of semantic search I hope you understood it and if you didn't understand it please drop me a comment below and I'll and I'll try to explain it myself so I'll try to give you more examples and maybe I can even just send you a couple of articles that explain this as well so with that we have the First Column of the first part the retrieve part which is actually the only complicated part because the augment and generate parts are very simple you'll see so by augment what we mean is just that once we have retrieved this memory so the closest one was his favorite movie is X minina we are going to inject that in the prompt uh along with the current conversation and those system instructions that AA or or the agent is following and finally once we have this complete prompt what we do is just to invoke a large language model in our case in the case of Ava we are using Lama 3.3 um via Gro and we just invoke this model to generate an answer and the answer could be something like xina right so this shows how we use H this framework rag rment generation to inject the relevant memories into aba's brain so let's review that again right so we take a query like remember my favorite movie we use an embedding model to generate an embedding and then we use a technique called semantic search to look in the vector database for those embeddings that are similar semantically we extract those or we retrieve those in this case could be something like his may his favorite movie is X minina then we aent right we inject this into the prompt and then we use now so now with all this information we call a l model to give you an answer which is something like X minina right awesome so this would more or less represent a retrieval augmented generation right but there are other Advanced things that I want to just quickly mention right which is this thing or this world con called chuning that you might have heard before and the whole idea of chuning is that you so in this case in the in the case of memories we usually have short um sentences or or short messages but what happens when you want to create a vector database based on books for example or long pieces of text so the whole idea is that you have to divide um divide or chunk those long text into shorter chunks so that would be chunking and also an invading model right so um in the same way that we have a a model for Generation we also have a model for embedding and here um you should select an embedding model depending on more um depending on that meaningful representation that want that you want to get from the data right so um an example could be for example if you are talking in in a very specific language right if you're talking in English then it's simple but maybe in Spanish there are models that work better than others so you would maybe use an embedding model that is better for only Spanish or for a multi-lingual application so that could be an example another example could be if you are um building a rag for a very specific um for a very specific domain for example biology or chemistry right so where you have all these technical words and maybe if you use a generic embeding model those would be diluted so you want a model that is able to find that meaningful representation between those um sentences and and terms and if you want a more detailed video about this Advanced Techniques and and just rag in general just let me know in the comments and I'll and I'll make it and now a very quick overview of vector datab base provider so as in the past lesson there are many to choose from so in the past lesson it was a IC Frameworks now we have Vector databases right so there are plenty I think I listed more or less the the ones that I know but I believe there are a couple more and again as with the identic Frameworks you shouldn't be using them all just choose the one that you like and just go for it because they all perform very similarly because they are all kind of like competing among themselves the things that you want to maybe be paying attention would be like the speed of the semantic search of this retrieval but I think more or less they are all equally equally fast um but I'm going to tell you why we chose quadrant over the other ones um I myself have used uh chroma in in a lot of projects but yeah for this project we use quadran for H three reasons so the first reason that is actually quite quite important for me is just that it's open source and I like when projects are open source quadrant it is H secondly you can selfhosted so that means that you don't actually need to pay for it so you can um have a so you can integrate that in your in your own server in your cluster in whatever in whatever you want and they have also a nice docket image for you to do it simply and finally because of their a free tier in the cloud option so they have a a cloudbased um option to use quadrant and they have a free tier um powered by Google Cloud which is very very convenient so you just need to um get an API key and just install a a pep library and and everything works and that's that's really a nice user experience or a nice developer experience and at the end of this video I'm going to show you how you can get thata and how the quadrant cloud looks like which is very very simple okay and let's now have a quick look at the code and I think by by really analyzing a couple of things that we have here written everything will make a lot of sense and and I promise you it is quite simple so um as for the shortterm memory I don't know if you remember in the first um in the first diagram that we saw in this lesson we had on the left side hand the memory right and we and I told you that we had that what we do is just essentially just writing down the the graph State um in in local this let's say by using um and the the graph state was this set of parameters the summary the workflow the AIO buffer so this is represented by the class AI companion state that that you have in the code which by the way just remember that you can access this code by looking at the report that I have linked Below in the description everything is completely open open source and you can have access to the entire uh application right now so in order to persist this graph state in memory which contains all those recent me messages and so on what we use is this um async SQ L saver and we use this as a as a shortterm memory right and just essentially as I said we um are this right now um locally right you could store this in a in a in a postgress SQL or whatever but just for the sake of this project we have decided to store it in memory and yeah essentially we just have this ex database leaving here and by using this we can recover those conversations or those recent messages or those or that graph state so what you have to do then is just use this checkpoint H parameter when you compile the graph and you give that a shortterm memory and that would be all quite simple as I said and now for the longterm memory remember remember that graph diagram in which we had the brain we had those two nodes we had the memory extraction node in which we extract those relevant pieces of information from the conversation and also the memory injection node which what it does essentially is this uh retrieval augmented generation framework so let's look first at the memory extraction Noe what we do is we just get this object that we have created called memory manager and we what we do is just extract and store those memories and we just pass that last message and if you look at the code you will see the it better but let me just jump into the most important thing which is maybe the prompt right so we have this method analyze memory in which we um create a prompt and then we invoke a message on that prompt and what we want to do is that the output of this invocation should be kind of like a memory anal is you're saying is this message important and how um and how the message should be formatted to be stored right and this goes along with the prompt and the prompt essentially does what I told you in the very beginning of this video which is to instruct the model on doing what I told you in the beginning which is um more or less what what what chpt does which is to understand in there are if there are important facts mentioned in the message things about personal personal details so the name the profession what are the preferences life circumstances so those things that are important will be saved as a memory so things that are that are important for example would be something like a message like how are you how are you doesn't doesn't convey or doesn't has doesn't have any relevant information so that would be discarded by this kind of prompt and then what we have is the memory injection node that what it does is the retrieval of those memories and then the injection right so we are still not doing here the generation but we are um using the memory manager to get those relevant Memories by using the vector database and then we are um returning this um as the graph right so this is kind of like the augmenting because now we have this context and it is then at the final stage when we use the conversation node to generate an answer that the memory context is used and we can quickly review H that prompt which is this one H in which you can see how as part of the the whole system instructions and so on we give um this right we give the memory context as the user background so here is where we will put all those H memories that are relevant to answer that specific query and for retrieving we have this method get relevant memories which uses the vector store or vector database to search for those memories and the search searching itself is just using a quadrant client to search over a collection and a collection is just as a table let's say in in in SQL so this would be more or less where you collect your your different embeddings and it's of course this search method the one that does this semantic search using the query embedding right the embeding of the query so we have to use an embedding model to encode that query and once we have that we also say how many K right so remember like this nearest nearest neighbor so how many um how many of those closed um closed vectors we want to retrieve and for this whole thing to work using quadrant Cloud the only thing that you have to do is just use the quadrant client which is um again it comes as a python library and you just need to give um the quadrant URL and the quad API key that I'm going to show you right now how you can get it so here we have quadrant Cloud when you are going to be able to see all your clusters and as I said in the beginning you have a free cluster powered by a Google cloud and yeah you simply you can just create it by just clicking create here right now I cannot create it because I have already one but once you create it you just give it a name and once you have that name you're going to be able to access the database by opening the dashboard here and this is going to show you all the collections that you have so for example we have the The Collection longterm memory of course for for this project and we have H different things right for for example I have things that ABA has gotten from my conversation so his s the Canary Islands but lives in Madrid well she well in this case it would be he but the whole idea of my message maybe my conversation with ABA was that I like to do CrossFit so ABA got okay he enjoys CrossFit because this it's something important maybe she will remember um a couple messages later when I when we talk about sports and here as well another thing you can see is just all the other Meta Meta data that you have along this Vector would be for example the time stamp or the length of the vector itself and uh finally um as I said you just need to set this up by just getting an API key and just setting the URL and that's it right so you create an API Key by just clicking here create I already have one and then you also uh use the endpoint or the quadrant URL and that's it you set that up as an environment variable and you are all set so that concluded lesson three and I hope you enjoy the video and if you did just please give it a like and also subscribe to my YouTube channel and hit them and hit the notification Bell if you want to be notified when we upload the next lesson which should be next week bye-bye